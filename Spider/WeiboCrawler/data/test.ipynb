{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets.csv\", encoding='utf-8')\n",
    "data[data.key_word==\"深圳天气\"].shape\n",
    "#data.key_word.unique()\n",
    "db = data[data.key_word==\"路况\"]\n",
    "contents = db.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for str in contents:\n",
    "    seg_list = jieba.cut(str, use_paddle=True)\n",
    "    print(\"Paddle Mode: \" + '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer, SementicRoleLabeller\n",
    "\n",
    "class LtpParser(object):\n",
    "    \"\"\" Initializing HIT-LTP project parsers. \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize ltp models by loading. \"\"\"\n",
    "        self.segmentor = Segmentor()\n",
    "        self.postagger = Postagger()\n",
    "        self.parser = Parser()\n",
    "        self.recognizer = NamedEntityRecognizer()\n",
    "        self.labeller = SementicRoleLabeller()\n",
    "        self.load()\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\" Load pre-train model \"\"\"\n",
    "        LTP_DIR=\"F:/Projects/NACTrans2020/ltp_models_v3.4.0\"\n",
    "        self.segmentor.load(os.path.join(LTP_DIR,\"cws.model\"))\n",
    "        self.postagger.load(os.path.join(LTP_DIR, \"pos.model\"))\n",
    "        self.parser.load(os.path.join(LTP_DIR,\"parser.model\"))\n",
    "        self.recognizer.load(os.path.join(LTP_DIR,\"ner.model\"))\n",
    "        self.labeller.load(os.path.join(LTP_DIR,\"pisrl_win.model\"))\n",
    "        print(\"INFO: Models loaded!\")\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\" Release ram space \"\"\"\n",
    "        self.segmentor.release()\n",
    "        self.postagger.release()\n",
    "        self.parser.release()\n",
    "        self.recognizer.release()\n",
    "        self.labeller.release()\n",
    "        print(\"INFO: Models released\")\n",
    "\n",
    "    def format_labelrole(self, words, postags):\n",
    "        \"\"\" Semantic role labelling using given modules.\"\"\"\n",
    "        arcs = self.parser.parse(words, postags)\n",
    "        roles = self.labeller.label(words, postags, arcs)\n",
    "        roles_dict = dict()\n",
    "        for role in roles:\n",
    "            roles_dict[role.index] = {arg.name:[arg.name, arg.range.start, arg.range.end] for arg in role.arguments}\n",
    "        return roles_dict\n",
    "\n",
    "    def build_parse_child_dict(self, words, postags,arcs):\n",
    "        \"\"\" Parsing by creating a corresponding child dict for each word within, to record dependency parsing result. \"\"\"\n",
    "        child_dict_list, format_parse_list = [], []\n",
    "        for index in range(len(words)):\n",
    "            child_dict = dict()\n",
    "            for arc_index in range(len(arcs)):\n",
    "                if arcs[arc_index] in range(len(arcs)):\n",
    "                    if arcs[arc_index].head == index+1:     # for arc_index starts from 1\n",
    "                        if arcs[arc_index].relation in child_dict: # search for current relations\n",
    "                            child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                        else:\n",
    "                            child_dict[arcs[arc_index].relation] =[]\n",
    "                            child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "            child_dict_list.append(child_dict)\n",
    "            \n",
    "        # extract parent nodes' ids.\n",
    "        rely_id = [ arc.head for arc in arcs ]\n",
    "        relation = [ arc.relation for arc in arcs ]\n",
    "        heads = ['Root' if id == 0 else words[id - 1] for id in rely_id]\n",
    "        for i in range(len(words)):\n",
    "            # output results.\n",
    "            a = [i, relation[i], words[i], postags[i], heads[i], rely_id[i] - 1, postags[rely_id[i] - 1]]\n",
    "            format_parse_list.append(a)\n",
    "\n",
    "        return child_dict_list, format_parse_list\n",
    "\n",
    "    def parser_main(self, sentence):\n",
    "        \"\"\" Main function \"\"\"\n",
    "        words = list(self.segmentor.segment(sentence))\n",
    "        postags = list(self.postagger.postag(words))\n",
    "        arcs = self.parser.parse(words, postags)\n",
    "        child_dict_list, format_parse_list = self.build_parse_child_dict(words, postags,arcs)\n",
    "        roles_dict = self.format_labelrole(words, postags)\n",
    "        return words, postags, child_dict_list, roles_dict, format_parse_list\n",
    "    \n",
    "#parse = LtpParser()\n",
    "#parse.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "原句：王子懿得到习的钦点，火速上任。 \n 分词：['王子懿', '得到', '习', '的', '钦点', '，', '火速', '上任', '。'] \n 词性：['nh', 'v', 'nh', 'u', 'n', 'wp', 'd', 'v', 'wp'] \n 依存子类字典: [{}, {}, {}, {}, {}, {}, {}, {}, {}] \n 语义标记：{1: {'A0': ['A0', 0, 0], 'A1': ['A1', 2, 4]}, 7: {'A0': ['A0', 0, 0], 'ADV': ['ADV', 6, 6]}} \n 格式化三元关系组：[[0, 'SBV', '王子懿', 'nh', '得到', 1, 'v'], [1, 'HED', '得到', 'v', 'Root', -1, 'wp'], [2, 'ATT', '习', 'nh', '钦点', 4, 'n'], [3, 'RAD', '的', 'u', '习', 2, 'nh'], [4, 'VOB', '钦点', 'n', '得到', 1, 'v'], [5, 'WP', '，', 'wp', '得到', 1, 'v'], [6, 'ADV', '火速', 'd', '上任', 7, 'v'], [7, 'COO', '上任', 'v', '得到', 1, 'v'], [8, 'WP', '。', 'wp', '得到', 1, 'v']]\n"
    }
   ],
   "source": [
    "#sentence = contents[0].replace(\"\\u200b\",' ').strip()\n",
    "sentence = \"王子懿得到习的钦点，火速上任。\"\n",
    "words, postags, child_dict_list, roles_dict, format_parse_list = parse.parser_main(sentence)\n",
    "print(\n",
    "    \"原句：{0} \\n 分词：{1} \\n 词性：{2} \\n 依存子类字典: {3} \\n 语义标记：{4} \\n 格式化三元关系组：{5}\".format(\n",
    "        sentence, words, postags, child_dict_list, roles_dict, format_parse_list\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class tripletableextractor(object):\n",
    "    \"\"\" Triple table extractor for long text contents. \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parser = LtpParser()\n",
    "\n",
    "    def split_contents(self, content):\n",
    "        \"\"\" Split long text contents by punctuations. \"\"\"\n",
    "        return [sentence for sentence in re.split(r'[#？?！!。；;：:\\n\\r]', content) if sentence]\n",
    "\n",
    "    def trunk_extractor(self, words, postags, roles_dict, role_index):\n",
    "        \"\"\" Triple extraction by searching for the trunk of sentences, using semantic role labelling. \"\"\"\n",
    "        v = words[role_index]\n",
    "        role_info = roles_dict[role_index]\n",
    "        if 'A0' in role_info.keys() and 'A1' in role_info.keys():\n",
    "            # Subject\n",
    "            s = ' '.join([words[word_index] for word_index in range(role_info['A0'][1], role_info['A0'][2]+1) if postags[word_index][0] not in ['w','u','x'] and words[word_index]])\n",
    "            # Object\n",
    "            o = ' '.join([words[word_index] for word_index in range(role_info['A1'][1], role_info['A1'][2]+1) if postags[word_index][0] not in ['w','u','x'] and words[word_index]])\n",
    "            if s and o:\n",
    "                return [s, v, o]\n",
    "\n",
    "        #  Reserved codes for double table extraction. (No-use currently)\n",
    "        # elif 'A0' in role_info:\n",
    "        #     s = ''.join([words[word_index] for word_index in range(role_info['A0'][1], role_info['A0'][2] + 1) if\n",
    "        #                  postags[word_index][0] not in ['w', 'u', 'x']])\n",
    "        #     if s:\n",
    "        #         return '2', [s, v]\n",
    "        # elif 'A1' in role_info:\n",
    "        #     o = ''.join([words[word_index] for word_index in range(role_info['A1'][1], role_info['A1'][2]+1) if\n",
    "        #                  postags[word_index][0] not in ['w', 'u', 'x']])\n",
    "        #     return '3', [v, o]\n",
    "\n",
    "        return []\n",
    "\n",
    "    def triple_extractor(self, words, postags, child_dict_list, arcs, roles_dict):\n",
    "        \"\"\" The main function for triple table extraction. \"\"\"\n",
    "        svos = []\n",
    "        for idx in range(len(postags)):\n",
    "            tmp = 1\n",
    "            # First, use semantic role labelling for extraction.\n",
    "            if idx in roles_dict:\n",
    "                triple = self.trunk_extractor(words, postags, roles_dict, idx)\n",
    "                if triple:\n",
    "                    svos.append(triple) \n",
    "                    tmp =0 \n",
    "            if tmp == 1:\n",
    "                # If returned empty triple, use dependency parsing for extraction.\n",
    "                if postags[idx]:\n",
    "                    # Extract triple centring around verbs.\n",
    "                    child_dict = child_dict_list[idx]\n",
    "                    \n",
    "                    # Direct relations\n",
    "                    if 'SBV' in child_dict and 'VOB' in child_dict:\n",
    "                        e1 = self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                        r = words[idx]\n",
    "                        e2 = self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                        svos.append([e1,r,e2])\n",
    "                    \n",
    "                    # Variant - attribute\n",
    "                    relation = arcs[idx][0]\n",
    "                    head = arcs[idx][2]\n",
    "                    if relation == 'ATT':\n",
    "                        if 'VOB' in child_dict:\n",
    "                            e1 = self.complete_e(words, postags, child_dict_list, head - 1)\n",
    "                            r = words[idx]\n",
    "                            e2 = self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                            tmp_string = r + e2\n",
    "                            if tmp_string == e1[:len(tmp_string)]:\n",
    "                                e1 = e1[len(tmp_string)]\n",
    "                            if tmp_string not in e1:\n",
    "                                svos.append([e1,r,e2])\n",
    "                    \n",
    "                    # Variant - complement & preposition-object\n",
    "                    if 'SBV' in child_dict and 'CMP' in child_dict:\n",
    "                        e1 = self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                        cmp_idx = child_dict['CMP'][0]\n",
    "                        r = words[idx] + words[cmp_idx]\n",
    "                        if 'POB' in child_dict_list[cmp_idx]:\n",
    "                            e2 = self.complete_e(words, postags, child_dict_list, child_dict_list[cmp_idx]['POB'][0])\n",
    "                            svos.append([e1,r,e2])\n",
    "        return svos\n",
    "\n",
    "    def complete_e(self, words, postags, child_dict_list, word_idx):\n",
    "        \"\"\" Extend subjects or objects found. \"\"\"\n",
    "        child_dict = child_dict_list[word_idx]\n",
    "        prefix = ''\n",
    "        postfix = ''\n",
    "        \n",
    "        # Extension mode 1 - Attribute\n",
    "        if 'ATT' in child_dict: \n",
    "            for idx in range(len(child_dict['ATT'])):\n",
    "                prefix += self.complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "\n",
    "        # Extension mode 2 - Further subject-verb or verb-object\n",
    "        if postags[word_idx] == 'v':\n",
    "            if 'VOB' in child_dict:\n",
    "                postfix += self.complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "            if 'SBV' in child_dict:\n",
    "                prefix += self.complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "        \n",
    "        return prefix + words[word_idx] + postfix\n",
    "\n",
    "    def triple_main(self, contents):\n",
    "        \"\"\" Main function for triple table extraction. \"\"\"\n",
    "        sentences = self.split_contents(contents)\n",
    "        svos = []\n",
    "        for sentence in sentences:\n",
    "            words , postags, child_dict_list, roles_dict, arcs = self.parser.parser_main(sentence)\n",
    "            svo = self.triple_extractor(words, postags, child_dict_list, arcs, roles_dict)\n",
    "            svos += svo\n",
    "\n",
    "        return svos\n",
    "    \n",
    "    def load(self):\n",
    "        self.parser.load()\n",
    "\n",
    "    def release(self):\n",
    "        self.parser.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "INFO: Models loaded!\n"
    }
   ],
   "source": [
    "extractor = tripletableextractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"text = 近日，一条男子高铁吃泡面被女乘客怒怼的视频引发热议。女子情绪激动，言辞激烈，大声斥责该乘客，称高铁上有规定不能吃泡面，质问其“有公德心吗”“没素质”。视频曝光后，该女子回应称，因自己的孩子对泡面过敏，曾跟这名男子沟通过，但对方执意不听，她才发泄不满，并称男子拍视频上传已侵犯了她的隐私权和名誉权，将采取法律手段。12306客服人员表示，高铁、动车上一般不卖泡面，但没有规定高铁、动车上不能吃泡面。\n",
    "                高铁属于密封性较强的空间，每名乘客都有维护高铁内秩序，不破坏该空间内空气质量的义务。这也是乘客作为公民应当具备的基本品质。但是，在高铁没有明确禁止食用泡面等食物的背景下，以影响自己或孩子为由阻挠他人食用某种食品并厉声斥责，恐怕也超出了权利边界。当人们在公共场所活动时，不宜过分干涉他人权利，这样才能构建和谐美好的公共秩序。\n",
    "                一般来说，个人的权利便是他人的义务，任何人不得随意侵犯他人权利，这是每个公民得以正常工作、生活的基本条件。如果权利可以被肆意侵犯而得不到救济，社会将无法运转，人们也没有幸福可言。如西谚所说，“你的权利止于我的鼻尖”，“你可以唱歌，但不能在午夜破坏我的美梦”。无论何种权利，其能够得以行使的前提是不影响他人正常生活，不违反公共利益和公序良俗。超越了这个边界，权利便不再为权利，也就不再受到保护。\n",
    "                在“男子高铁吃泡面被怒怼”事件中，初一看，吃泡面男子可能侵犯公共场所秩序，被怒怼乃咎由自取，其实不尽然。虽然高铁属于封闭空间，但与禁止食用刺激性食品的地铁不同，高铁运营方虽然不建议食用泡面等刺激性食品，但并未作出禁止性规定。由此可见，即使食用泡面、榴莲、麻辣烫等食物可能产生刺激性味道，让他人不适，但是否食用该食品，依然取决于个人喜好，他人无权随意干涉乃至横加斥责。这也是此事件披露后，很多网友并未一边倒地批评食用泡面的男子，反而认为女乘客不该高声喧哗。\n",
    "                现代社会，公民的义务一般分为法律义务和道德义务。如果某个行为被确定为法律义务，行为人必须遵守，一旦违反，无论是受害人抑或旁观群众，均有权制止、投诉、举报。违法者既会受到应有惩戒，也会受到道德谴责，积极制止者则属于应受鼓励的见义勇为。如果有人违反道德义务，则应受到道德和舆论谴责，并有可能被追究法律责任。如在公共场所随地吐痰、乱扔垃圾、脱掉鞋子、随意插队等。此时，如果行为人对他人的劝阻置之不理甚至行凶报复，无疑要受到严厉惩戒。\n",
    "                当然，随着社会的发展，某些道德义务可能上升为法律义务。如之前，很多人对公共场所吸烟不以为然，烟民可以旁若无人地吞云吐雾。现在，要是还有人不识时务地在公共场所吸烟，必然将成为众矢之的。\n",
    "                再回到“高铁吃泡面”事件，要是随着人们观念的更新，在高铁上不得吃泡面等可能产生刺激性气味的食物逐渐成为共识，或者上升到道德义务或法律义务。斥责、制止他人吃泡面将理直气壮，否则很难摆脱“矫情”，“将自我权利凌驾于他人权利之上”的嫌疑。\n",
    "                在相关部门并未禁止在高铁上吃泡面的背景下，吃不吃泡面系个人权利或者个人私德，是不违反公共利益的个人正常生活的一部分。如果认为他人吃泡面让自己不适，最好是请求他人配合并加以感谢，而非站在道德制高点强制干预。只有每个人行使权利时不逾越边界，与他人沟通时好好说话，不过分自我地将幸福和舒适凌驾于他人之上，人与人之间才更趋于平等，公共生活才更趋向美好有序。\"\"\"\n",
    "text = contents[0]\n",
    "svos = extractor.triple_main(text)\n",
    "print(\"svos: {}\".format(svos))\n",
    "extractor.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'operator.methodcaller'>\n<class 'operator.methodcaller'>\n<class 'operator.methodcaller'>\n<class 'operator.methodcaller'>\n"
    }
   ],
   "source": [
    "from operator import methodcaller\n",
    "from tqdm import tqdm\n",
    "class test(object):\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.b =2 \n",
    "        self.c = 3\n",
    "        self.d = 4\n",
    "    \n",
    "    def tqdm_bar(self):\n",
    "        for  (var,index) in tqdm(vars(self).items()):\n",
    "            print('\\n',index,var)\n",
    "\n",
    "    def type_tester(self):\n",
    "        load = methodcaller('load')\n",
    "        for var in vars(self):\n",
    "            print(type(methodcaller(var)))\n",
    "\n",
    "t = test()\n",
    "t.type_tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Help on class methodcaller in module operator:\n\nclass methodcaller(builtins.object)\n |  methodcaller(name, ...) --> methodcaller object\n |  \n |  Return a callable object that calls the given method on its operand.\n |  After f = methodcaller('name'), the call f(r) returns r.name().\n |  After g = methodcaller('name', 'date', foo=1), the call g(r) returns\n |  r.name('date', foo=1).\n |  \n |  Methods defined here:\n |  \n |  __call__(self, /, *args, **kwargs)\n |      Call self as a function.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  __reduce__(...)\n |      Return state information for pickling\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n\n"
    }
   ],
   "source": [
    "help(methodcaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "_date:  2016-12-01\nmax_temp:  22\nmin_temp:  16\nweather:  多云\nwind_dir:  北风 2级\n\n\n_date:  2016-12-02\nmax_temp:  23\nmin_temp:  17\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-03\nmax_temp:  23\nmin_temp:  18\nweather:  多云\nwind_dir:  西北风 1级\n\n\n_date:  2016-12-04\nmax_temp:  24\nmin_temp:  19\nweather:  多云\nwind_dir:  东北风 1级\n\n\n_date:  2016-12-05\nmax_temp:  26\nmin_temp:  18\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-06\nmax_temp:  22\nmin_temp:  15\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-07\nmax_temp:  22\nmin_temp:  16\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-08\nmax_temp:  25\nmin_temp:  16\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-09\nmax_temp:  23\nmin_temp:  16\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-10\nmax_temp:  24\nmin_temp:  17\nweather:  多云\nwind_dir:  南风 1级\n\n\n_date:  2016-12-11\nmax_temp:  25\nmin_temp:  18\nweather:  多云\nwind_dir:  东风 2级\n\n\n_date:  2016-12-12\nmax_temp:  25\nmin_temp:  19\nweather:  多云\nwind_dir:  东风 2级\n\n\n_date:  2016-12-13\nmax_temp:  27\nmin_temp:  16\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-14\nmax_temp:  22\nmin_temp:  13\nweather:  多云\nwind_dir:  西北风 2级\n\n\n_date:  2016-12-15\nmax_temp:  19\nmin_temp:  12\nweather:  晴\nwind_dir:  北风 2级\n\n\n_date:  2016-12-16\nmax_temp:  19\nmin_temp:  11\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-17\nmax_temp:  20\nmin_temp:  12\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-18\nmax_temp:  22\nmin_temp:  17\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-19\nmax_temp:  26\nmin_temp:  19\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-20\nmax_temp:  25\nmin_temp:  19\nweather:  阴\nwind_dir:  东北风 1级\n\n\n_date:  2016-12-21\nmax_temp:  25\nmin_temp:  17\nweather:  小雨\nwind_dir:  北风 1级\n\n\n_date:  2016-12-22\nmax_temp:  24\nmin_temp:  14\nweather:  多云\nwind_dir:  北风 2级\n\n\n_date:  2016-12-23\nmax_temp:  20\nmin_temp:  16\nweather:  多云\nwind_dir:  东风 1级\n\n\n_date:  2016-12-24\nmax_temp:  20\nmin_temp:  16\nweather:  小雨\nwind_dir:  北风 2级\n\n\n_date:  2016-12-25\nmax_temp:  26\nmin_temp:  19\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-26\nmax_temp:  24\nmin_temp:  13\nweather:  多云\nwind_dir:  北风 2级\n\n\n_date:  2016-12-27\nmax_temp:  17\nmin_temp:  10\nweather:  晴\nwind_dir:  北风 3级\n\n\n_date:  2016-12-28\nmax_temp:  15\nmin_temp:  9\nweather:  多云\nwind_dir:  北风 2级\n\n\n_date:  2016-12-29\nmax_temp:  15\nmin_temp:  10\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-30\nmax_temp:  18\nmin_temp:  13\nweather:  多云\nwind_dir:  北风 1级\n\n\n_date:  2016-12-31\nmax_temp:  21\nmin_temp:  16\nweather:  多云\nwind_dir:  东南风 1级\n\n\n_date:  \nmax_temp:  \nmin_temp:  \nweather:  \nwind_dir:  \n\n\n"
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "url = \"https://lishi.tianqi.com/shenzhen/201612.html\"\n",
    " \n",
    "headers = {\n",
    "    'User-Agent': \"PostmanRuntime/7.16.3\",\n",
    "    'Accept': \"*/*\",\n",
    "    'Cache-Control': \"no-cache\",\n",
    "    'Host': \"lishi.tianqi.com\",\n",
    "    'Accept-Encoding': \"gzip, deflate\",\n",
    "    'Connection': \"keep-alive\",\n",
    "    'cache-control': \"no-cache\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "tree_node = etree.HTML(response.text)\n",
    "records = tree_node.xpath('//*[@class=\"lishitable_content clearfix\"]/li')\n",
    "for record in records:\n",
    "    try:\n",
    "        print('_date: ', record.xpath('string(.//a/text())'))\n",
    "        print('max_temp: ', record.xpath('string(.//div[2]/text())'))\n",
    "        print('min_temp: ', record.xpath('string(.//div[3]/text())'))\n",
    "        print('weather: ', record.xpath('string(.//div[4]/text())'))\n",
    "        print('wind_dir: ', record.xpath('string(.//div[5]/text())'))\n",
    "        print('\\n')\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('nactrans': conda)",
   "language": "python",
   "name": "python36864bitnactransconda6b19a775b2d24e34808c0e57e462e3b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.6.8-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}